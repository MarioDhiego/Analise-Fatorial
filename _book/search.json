[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ANÁLISE FATORIAL: Um Guia Prático no R",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Análise Fatorial - AF",
    "section": "",
    "text": "1.1 Aspectos Históricos\nHistoricamente, a origem das técnicas de análise de fatores ou análise fatorial está ligada a estudos da área de psicologia. Sua criação data do início do século XX, quando Karl Pearson (1901) e Charles Spearman (Spearman, 1904) desenvolveram um método para a criação de um índice geral de inteligência (fator “g”) com base nos resultados em vários testes (escalas) que refletiriam essa aptidão. Tratava-se de um primeiro método de AF, adequado para a estimação de um único fator. No entanto, o termo Análise de Fatorial foi introduzido por Louis L. Thurstone em 1931 no seu artigo Multiple Factor Analysis.\nThurstone (1931) identificou sete habilidades mentais primárias, em vez de um único fator g. Estudos mais recentes têm alterado a quantidade de fatores a serem considerados na análise de inteligência. No início, os métodos apresentavam uma característica mais empírica do que inferencial. Em 1933 Pearson e Hotteling deram um formalismo melhor as idéias criadas por Spearman, e assim impulsionado com essas idéias, em 1940 com Lawley, surge um primeiro trabalho com um maior rigor matemático, o que fez com que se aumentasse à aceitação dessas técnicas no ramo da psicologia (LAWLEY, 1940).\nCertos conceitos das ciências sociais e comportamentais não são bem definidos e existem muitas discussões sobre o real significado de termos como classe social, opinião pública, comportamento de risco ou personalidade extrovertida. Tais conceitos são freqüentemente chamados de variáveis latentes, desde que não são diretamente observáveis mesmo na população. Trata-se de construtos inventados pelos cientistas com o propósito de entender alguma área de interesse na pesquisa sendo realizada e para a qual não existe método operacional para fazer uma medida de forma direta.\nEmbora as variáveis latentes não possam ser observadas diretamente, alguns de seus efeitos aparecerão nas variáveis manifestas, ou seja, aquelas que podem ser verificadas. Fica claro que medir diretamente um conceito como preconceito racial não é possível; no entanto, pode-se, por exemplo, observar quando uma pessoa aprova, ou não, alguma legislação do governo a respeito deste assunto. Pode-se, também, saber de que raça são os amigos desta pessoa e assim assumir que tais observações são de algum modo, indicadores de uma variável mais fundamental, o preconceito racial (Everitt, 1984). O método mais conhecido para investigar a dependência de um conjunto de variáveis manifestas em relação a um número menor de variáveis latentes é a chamada Análise de Fatores.\nA análise de fatores é uma das técnicas mais usuais do que se convencionou chamar de análise multivariada. Quando empregamos este tipo de análise estamos frequentemente interessados no comportamento de uma variável ou grupos de variáveis em covariação com outras (GREEN, 1976).\nEm realidade a análise fatorial não se refere a uma única técnica estatística, mas a uma variedade de técnicas relacionadas para tornar os dados observados mais facilmente (e diretamente) interpretados. Isto é feito analisando-se os inter-relacionamentos entre as variáveis de tal modo que estas possam ser descritas convenientemente por um grupo de categorias básicas, em número menor que as variáveis originais, chamado fatores. Assim, o objetivo da análise fatorial é a parcimônia, procurando definir o relacionamento entre as variáveis de modo simples e usando um número de fatores menor que o número original de variáveis.\nMais precisamente, um fator é um construto, uma entidade hipotética, uma variável não observada, que se supõe estarem subjacente a testes, escalas, itens e, de fato, medidas de qualquer espécie. Como construtos, os fatores apenas possuem realidade no fato de explicarem a variância de variáveis observadas, tal como se revelam pelas correlações entre as variáveis sendo analisadas, ou seja, a única realidade científica que os fatores possuem vem das correlações entre testes ou variáveis sendo pesquisadas. Se os resultados de indivíduos em itens ou testes caminham juntos, então, na medida em que existam correlações substanciais entre eles, está definido um fator.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#definição-clássica---af",
    "href": "intro.html#definição-clássica---af",
    "title": "1  Análise Fatorial - AF",
    "section": "1.2 Definição Clássica - AF",
    "text": "1.2 Definição Clássica - AF\nA análise fatorial (AF), de modo geral, é uma técnica estatística multivariada que tem como princípio analisar a estrutura das inter-relações (correlações) entre um grande número de variáveis, ou seja, descrever a estrutura de dependência de um conjunto de variáveis através da criação de fatores, que são variáveis que, supostamente, medem aspectos comuns (Hair et al, 2005). Com o emprego dessa técnica, inicialmente podem-se identificar as dimensões isoladas da estrutura dos dados e então determinar o grau em que cada variável é explicada por cada dimensão ou fator.\nSegundo Mingoti (2005), a AF tem como objetivo principal descrever a variabilidade original do vetor aleatório X, em termos de um número menor M de variáveis aleatórias, chamadas de fatores comuns e que estão relacionadas com o vetor original X através de um modelo linear. Neste modelo, parte da variabilidade de X, é atribuída aos fatores comuns, sendo o restante da variabilidade de X atribuído ás variáveis que não foram incluídas no modelo, ou seja, o erro aleatório.\nPara Barroso e Artes (2003), a AF é uma técnica que descreve a estrutura de dependência de um conjunto de variáveis, através da criação de fatores, que são variáveis que, supostamente, medem aspectos comuns.\nReis (1997) descreve a AF como um conjunto de técnicas cuja finalidade é representar ou descrever um número de variáveis iniciais a partir de um menor número de variáveis hipotéticas. Trata-se de uma técnica estatística multivariada que, a partir da estrutura de dependência existente entre as variáveis de interesse (em geral representada pelas correlações ou covariâncias entre essas variáveis), permite a criação de um conjunto menor de variáveis (variáveis latentes ou fatores), obtidas a partir das originais.\nDe acordo com Malhotra (2001), análise fatorial é um nome genérico que denota uma classe de processos essencialmente para redução e sumarização dos dados”.\nA análise fatorial é uma técnica de interdependência na qual todas as variáveis são simultaneamente consideradas, cada uma relacionada com todas as outras, empregando o conceito da variável estatística, a composição linear das variáveis. Ela difere das técnicas multivariadas de dependência (Regressão Múltipla, Análise Discriminante, Análise Multivariada de Variância ou Correlação Canônica), nas quais uma ou mais variáveis são explicitamente consideradas como as variáveis de critério ou dependentes e todas as outras são as variáveis preditoras ou independentes.\nNa análise fatorial, as variáveis estatísticas (fatores) são formadas para maximizar seu poder de explicação do conjunto inteiro de variáveis, e não para prever uma variável (eis) dependente(s).\nNa concepção de Pasquali (2003), a análise fatorial é uma técnica caucada sobre o pressuposto de que uma série de variáveis observadas, medidas, chamadas de variáveis empíricas ou observáveis pode ser explicada por um número menor de variáveis não observáveis, chamadas precisamente de variáveis fontes, mais conhecidas sob o nome de fatores.\nAs chamadas variáveis fontes seriam a causa do fato de que as variáveis observáveis se relacionam entre si, isto é, são responsáveis pelas intercorrelações (covariâncias) entre as variáveis. Supõe-se que, se as variáveis empíricas se relacionam entre si, é porque elas têm uma causa comum que produz esta correlação entre elas. E a esta causa comum que se chama de fator e cuja descoberta é precisamente a tarefa da análise fatorial. Então, nestas afirmações já fizemos dois postulados que a análise fatorial assume (Pasquali, 2003):\n\n\nPrincípio da Parcimônia ou de Rank Reduction: Um número menor de variáveis-fonte é suficiente para explicar uma série maior de variáveis observáveis, isto é, redução do posto da matriz das intercorrelações entre as variáveis observáveis.\n\n\nPrincípio da Causalidade: As variáveis-fonte são as causas da covariância entre as variáveis observáveis, ou seja, a análise fatorial é um modelo causal.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#tipos-de-análise-fatorial",
    "href": "intro.html#tipos-de-análise-fatorial",
    "title": "1  Análise Fatorial - AF",
    "section": "1.3 Tipos de Análise Fatorial",
    "text": "1.3 Tipos de Análise Fatorial\nTécnicas analíticas fatoriais podem atingir seus objetivos ou de uma perspectiva exploratória ou de uma confirmatória. Existe um debate contínuo sobre o papel apropriado da análise fatorial, onde muitos pesquisadores consideram a AF apenas exploratória, útil na busca da estrutura em um conjunto de variáveis ou como um método de redução de dados. Sob essa perspectiva, as técnicas analíticas fatoriais consideram o que os dados oferecem e não estabelecem restrições a priori sobre o número de componentes a serem extraídos. Para muitas, talvez a maioria das aplicações, esse uso da AF é adequado. No entanto, em outras situações, o pesquisador tem preconcebido idéias sobre a real estrutura dos dados, baseado em suporte teórico ou em pesquisas anteriores. Ele pode desejar testar hipóteses envolvendo questões sobre, por exemplo, quais variáveis deveriam ser agrupadas em fator ou número exato de fatores. Nesses casos, o pesquisador espera que a análise fatorial desempenhe um papel confirmatório, ou seja, avalie o grau em que os dados satisfazem à estrutura esperada.\n\n1.3.1 Análise Fatorial Exploratória - AFE\nÈ comum um pesquisador analisar dados relativos a um fenômeno sem dispor de um quadro de referência teórica que o oriente completamente. Na maioria das vezes, essa situação decorre da falta de investimento na estruturação teórica do problema, da existência de lacunas na teoria ou da falta de ajuste de um modelo teórico proposto de antemão aos dados observados.\nDe qualquer modo, sempre deve haver um mínimo de teoria indispensável à utilização da AF, mesmo em sua forma exploratória. No mínimo, o pesquisador deve supor que, por trás das variáveis observadas, há uma estrutura de fatores ortogonais ou oblíquos. Isso por si só não é pouco, pois exclui relações causais entre fatores, relações causais entre variáveis observadas e correlações dos fatores únicos com outros fatores ou com variáveis cuja especificidade não determina.\nAssumida a existência de uma estrutura fatorial subjacente, o objetivo primário da Análise Fatorial Exploratória é determinar a menor quantidade possível de fatores que possam reproduzir a estrutura de correlações das variáveis observadas. Secundariamente, o propósito é ajustar soluções em que um pequeno número de variáveis tenha carga elevada em cada fator, pois isso facilita a interpretação da solução.\n\n\n1.3.2 Análise Fatorial Confirmatória - AFC\nA aplicação na análise fatorial confirmatória procura verificar se os dados observados se comportam de acordo com uma expectativa teórica. Se isso ocorrer, serve como evidência favorável à validade dos dados e reforça a teoria proposta, se não acontecer, alerta para existência de problemas com os dados, com a teoria ou com ambos. O requisito mínimo para execução da análise fatorial confirmatória é o pesquisador possuir de antemão uma hipótese acerca da quantidade de fatores comuns e apresentar uma expectativa teórica sobre qual fator deve carregar em qual variável.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#cargas-fatoriais",
    "href": "intro.html#cargas-fatoriais",
    "title": "1  Análise Fatorial - AF",
    "section": "1.4 Cargas Fatoriais",
    "text": "1.4 Cargas Fatoriais\nAs cargas fatoriais obtidas são, com efeito, reduções de dados muito mais complexos a tamanho manuseável para que o pesquisador possa interpretar melhor os resultados (KERLINGER, 1980).\nA expressão carga fatorial ocorre freqüentemente. Uma matriz de cargas fatoriais é um dos produtos finais da análise fatorial. Uma carga fatorial é um coeficiente, um número decimal, positivo ou negativo, geralmente menor do que 1, que expressa o quanto um teste ou variável observada está carregado ou saturado em um fator. Por outras palavras, quanto maior for a carga em cima de um fator, mais a variável se identifica com o que quer que seja o fator.\nEm síntese, a análise fatorial é essencialmente um método para determinar o número de fatores existentes em um conjunto de dados, para determinar quais testes ou variáveis pertencem a quais fatores, e em que extensão os testes ou variáveis pertencem a/ou estão saturados com o que quer que seja o fator.\nAssim, na AF uma situação, com inúmeras variáveis (\\(X_{1}, X_{2},..., X_{p}\\)), é explicada a partir de dimensões escondidas (fatores: \\(F_{1}, F_{2}, F_{3}\\)). A Figura 1 demonstra graficamente o esquema geral da análise de fatores.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#modelo-estatístico---af",
    "href": "intro.html#modelo-estatístico---af",
    "title": "1  Análise Fatorial - AF",
    "section": "1.5 Modelo Estatístico - AF",
    "text": "1.5 Modelo Estatístico - AF\nO modelo estatístico usado na análise fatorial explica uma estrutura de correlação existente entre um conjunto de informações, diretamente observado por meio de combinação linear de variáveis, as quais não são diretamente observadas, denominados fatores comuns, acrescidas de componente residual. Um modelo de análise fatorial pode ser apresentado na seguinte forma conforme (DILLON e GOLDSTEIN, 1984).\nSejam variáveis aleatórias, m fatores comuns; m&lt;p.\n\\[  X_{i} = \\sum_{j=1}^{m} \\lambda_{ij} F_{j} + \\epsilon_{i} \\]\nEm que:\n\\(X_{i}\\) = variáveis originais ou observadas;\n\\(\\lambda_{ij}\\) = Cargas fatoriais da i-ésima variável no j-ésimo fator comum e refletem a importância do j-ésimo fator na composição da i-ésima variável.\n\\(F_{j}\\) = variáveis não-observáveis ou variáveis latentes chamadas de fatores comuns;\n\\(\\epsilon_{i}\\) = Fatores específicos que descrevem a variação residual específica da i-ésima variável (resíduo que afeta somente \\(X_{i}\\)). É a parte da variável \\(X_{i}\\) que não é explicada pelos fatores comuns.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#comunalidades",
    "href": "intro.html#comunalidades",
    "title": "1  Análise Fatorial - AF",
    "section": "1.6 Comunalidades",
    "text": "1.6 Comunalidades\nA porção da variância que a variável contribui para o fator comum m é chamada de comunalidade. A porção da variância Var(X)= \\(\\sigma^{2}\\) devido ao fator específico, chama-se especificidade ou variância específica.\nEntão tem-se que A variância de cada variável \\(X_{i}\\) é a soma das Comunalidades \\(h_{i}^{2}\\) com suas respectivas especificidades ou variância específica \\(\\psi_{i}\\).\n\\[\\underbrace{\\sigma_{ij}}_{\\mbox{Var} (X_{i})} = \\underbrace{l^{2}_{i1}+l^{2}_{i2}+\\ldots+l^{2}_{im}}_{\\mbox{Comunalidades}}  +\\underbrace{\\psi_{i}}_{\\mbox{Variancia Especifica}}\\] Denotando-se a i-ésima comunalidade por \\(h_{i}^{2}\\) tem-se:\n\\[h_{ij}^{2} =  l^{2}_{i1}+l^{2}_{i2}+\\ldots+l^{2}_{im} = \\sum_{ij}^{m} l^{2}_{ij} \\ \\ \\ \\ \\ com \\ \\ \\ \\ i=1,2,...,p \\]\nEntão:\n\\[ \\sigma_{ij}  = h_{i}^{2} + \\psi_{i}\\]\nA i-ésima comunalidade é a soma dos quadrados dos carregamentos da i-ésima variável com m fatores comuns. Portanto as comunalidades são as maiores parcelas do total da variância de uma variável \\(X_{i}\\). A segunda parcela é proveniente da variância específica de cada variável, representada por \\(\\psi\\). Quanto mais a comunalidade se aproximar de 1, melhor será o modelo fatorial. Autores consideram boa comunalidade valores acima de 0,70.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#principais-etapas-para-aplicação---af",
    "href": "intro.html#principais-etapas-para-aplicação---af",
    "title": "1  Análise Fatorial - AF",
    "section": "1.7 Principais Etapas para Aplicação - AF",
    "text": "1.7 Principais Etapas para Aplicação - AF\nA análise fatorial possui, basicamente, oito etapas para a sua elaboração que são:\n\nFormular o Problema: Formulação do problema da análise fatorial e identificação das variáveis originais que serão investigadas para resumo ou redução dos dados.\nCálculo da Matriz de Correlação: Cálculo da matriz de correlação das variáveis em estudo para a verificação do grau de associação entre as variáveis. Para testar a conveniência do modelo fatorial, pode-se aplicar o teste de esfericidade de Bartlett para testar a hipótese nula de que as variáveis não sejam correlacionadas na população, ou seja, que a matriz de correlação da população é uma matriz de identidade. Outra estatística utilizada pode ser a medida de adequacidade da amostra de Kaiser-Meyer-Olkin (KMO), um índice usado para avaliar a medida de adequacidade da amostra da análise fatorial. Valores altos (entre 0,5 e 1,0) indicam que a análise fatorial é apropriada. Assim, a partir de uma análise da matriz de correlação das diversas variáveis, é possível obter indicadores sintéticos, ou, utilizando o termo técnico, escores fatoriais, que consistem numa combinação linear das variáveis originais que as sintetizam e explicam (KAISER E RICE, 1974, SPSS, 1999).\nMétodo de Análise: Escolha do método de análise fatorial mais apropriado para extração dos fatores a serem utilizados existem vários dentre os mais usuais são: Método de Componentes Principais, Método de Análise Fatorial Comum e Método de Máxima Verossimilhança.\nExtrair Fatores Iniciais: Extração dos fatores (cargas fatoriais) mais significativos que representarão os dados, através do método mais adequado. Podem ser: a priori, com base em autovalores, com base em um gráfico de Declive (Scree Plot), na percentagem da variância. Aqui se sabe o quão bem o modelo representa os dados (HAIR et. al, 2005).\nRotação dos Fatores: Aplicar algum tipo de rotação nos fatores para facilitar o entendimento dos mesmos. Podem ser: ortogonal, quando os eixos são mantidos em ângulo reto e rotação oblíqua, quando os eixos não são mantidos em ângulo reto. (Geralmente, utiliza-se rotação para transformar a matriz de fatores em uma matriz mais simples e mais fácil de interpretar. O método de rotação mais comumente usado é o processo varimax que objetiva diminuir o número de variáveis com cargas fatoriais altas em um só fator, resultando em fatores ortogonais. Se há fatores altamente correlacionados na população pesquisada, pode-se usar a rotação oblíqua).\nInterpretação de Fatores: É facilitada através da identificação das variáveis que apresentam grandes cargas sobre o mesmo fator. O fator pode então ser interpretado em termos das variáveis que o pesam fortemente\nCálculo dos Escores Fatoriais: Escores fatoriai são estimados para cada observação nos fatores derivados. A redução de dados pode ser conseguida calculando escores para cada dimensão latente e substituindo as variáveis originais pelos mesmos. A análise fatorial pode auxiliar na seleção de um subconjunto representativo de variáveis ou mesmo da criação de novas variáveis como substitutas das variáveis originais, e ainda mantendo seu caráter original.\nDeterminação do Ajuste do Modelo: As diferenças entre as correlações observadas (matriz de entrada) e as correlações reproduzidas (estimada com base na matriz de fatores) são chamadas de resíduos e se há muitos resíduos grandes, o modelo fatorial não dá um bom ajuste aos dados, e deve ser reconsiderado.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#método-de-extração-dos-fatores",
    "href": "intro.html#método-de-extração-dos-fatores",
    "title": "1  Análise Fatorial - AF",
    "section": "1.8 Método de Extração dos Fatores",
    "text": "1.8 Método de Extração dos Fatores\nHá na literatura vários métodos para a estimação de fatores tais como:\n\nMethod Unweighted Least Squares: um dos métodos de extração que minimiza a soma das diferenças quadráticas entre a matriz de dados e a matriz de correlação reproduzida, ignorando as diagonais.\nMethod Generalized Least Squares: idem ao anterior, mas neste caso a correlação é pesada pelo inverso das suas singularidades, assim como as variáveis com altas singularidades são tomadas com peso menor que aquelas com menor singularidades.\nMethod Maximun Likelihood: este cria parâmetros estimados como sendo mais prováveis para produzir a matriz de correlação observada, se a amostra pode ser caracterizada por uma distribuição normal multivariada. As correlações são pesadas pelo inverso das singularidades das variáveis, pelo emprego de um algoritmo “iterativo”.\nMethod Principal Axis Factoring: parte da matriz de correlação original com os coeficientes de correlações múltiplos colocados na diagonal como estimativas iniciais das comunalidades.\nMethod Principal Components: é usada para obter uma combinação linear não-correlata das combinações das variáveis mensuradas, obtendo-se as soluções dos fatores, ela pode ser usada quando a matriz de correlação é singular.\nMethod Alpha Factoring: é um método de extração que considera as variáveis na análise como uma amostra do universo potencial de variáveis. Ele maximiza a confiabilidade ou fidedignidade alfa (de Cronbach) dos fatores\nMethod Image Factoring: é um método fatorial de extração desenvolvido por Guttman e está baseado na Teoria de Imagens. A parte comum da variância, chamada de imagem parcial, é definida como uma regressão linear sobre as restantes, preferivelmente que a função dos fatores hipotéticos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#método-de-rotação-dos-fatores",
    "href": "intro.html#método-de-rotação-dos-fatores",
    "title": "1  Análise Fatorial - AF",
    "section": "1.9 Método de Rotação dos Fatores",
    "text": "1.9 Método de Rotação dos Fatores\nUma ferramenta importante na interpretação de fatores é a rotação fatorial. O termo rotação significa exatamente o que sugere. Especificamente, os eixos de referência dos fatores são rotacionados em torno da origem até que alguma outra posição seja alcançada. As soluções de fatores não-rotacionadas extraem fatores na ordem de sua importância.\nO primeiro fator tende a ser um fator geral com quase toda a variável com carga significativa, e explica a quantia maior de variância. O segundo fator e os seguintes são então baseados na quantia residual de variância. Cada fator explica porções sucessivamente menores de variância. O efeito final de rotacionar a matriz fatorial é redistribuir a variância dos primeiros fatores para os últimos com o objetivo de atingir um padrão fatorial mais simples e teoricamente mais significativo.\nO caso mais simples de rotação é uma Rotação Ortogonal, na qual os eixos são mantidos a 90º. Também é possível rotacionar os eixos sem manter o ângulo de 90 graus entre os eixos de referência. Procedimento de rotação se chama **Rotação Oblíqua*.\n\n1.9.1 Rotação Ortogonal dos Fatores\nPara rotação ortogonal dos fatores existem vários tipos na literatura como: Varimax (mais usado) que é um método de rotação ortogonal que minimiza o número de variáveis que cada agrupamento terá. Ele simplifica a interpretação dos fatores.\nQuartimax é um método que minimiza o número de fatores necessários para explicar cada variável. Ele simplifica a interpretação das variáveis obtidas.\nO Equamax é também um método que busca uma combinação dos outros (varimax e quartimax). O número de variáveis obtido terá carga fatorial maior e o número de fatores será minimizado.\n\n1.9.1.1 Rotação Varimax\nA rotação varimax é uma das rotações ortogonais mais utilizadas em análise fatorial. Intuitivamente, ela busca soluções nas quais se busca maximizar as correlações de cada variável com apenas um fator.\nO método varimax é um processo em que os eixos de referência dos fatores são rotacionados em torno da origem até que alguma outra posição seja alcançada. Intuitivamente, ela busca soluções nas quais se busca maximizar as correlações de cada variável com apenas um fator. O objetivo é redistribuir a variância dos primeiros fatores para os demais e atingir um padrão fatorial mais simples e teoricamente mais significativo (REIS, 2001; HAIR, 2005).\nVarimax é um tipo de rotação ortogonal na qual mantém os fatores perpendiculares entre si, ou seja, sem correlação entre eles. È o tipo de rotação mais utilizado e que tem como característica o fato de minimizar a ocorrência de uma variável possuir altas cargas fatoriais para diferentes fatores, permitindo que uma variável seja facilmente identificada como único fator (CORRAR et al, 2007).\n\n\n\n1.9.2 Rotação Oblíqua dos Fatores\nPara rotação oblíqua dos fatores cita-se: Direct Oblimin um método diferentemente dos três anteriores é oblíquo (não ortogonal). Quando delta é igual a 0 (default), a solução é mais oblíqua. Tomando-se delta mais negativo, os fatores ficaram menos oblíquos. Ignorando-se o default delta de 0, deve-se usar um número menor ou igual a 0,8.\nO Promax também é um método oblíquo de rotação, o qual possibilita os fatores correlatos. Ele pode ser calculado mais rapidamente que a rotação direct oblimin. Assim ele é usado para grandes grupos de dados. Kappa na maioria das vezes é tomado com o valor 4. Existem outros métodos de rotação oblíqua na literatura como:\n\nCovarimin;\nOblimax;\nQuartimin\n\nmas que não são tão usados como os anteriores mensionados.\nOs mesmos princípios gerais de rotações ortogonais são aplicáveis a rotações oblíquas. O método de rotação oblíqua é mais flexível, pois os eixos fatoriais não precisam ser ortogonais. Além disso, é mais realista porque as dimensões inerentes que são teoricamente importantes não são supostas sem correlações entre si. A solução oblíqua fornece informações sobre o grau em que os fatores realmente estão correlacionados um com o outro.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#métodos-de-escolha-do-número-de-fatores",
    "href": "intro.html#métodos-de-escolha-do-número-de-fatores",
    "title": "1  Análise Fatorial - AF",
    "section": "1.10 Métodos de Escolha do Número de Fatores",
    "text": "1.10 Métodos de Escolha do Número de Fatores\nA escolha do número de fatores é uma das tarefas mais importante de uma análise fatorial. Quando o pesquisador opta por um número muito reduzido, ele pode não identificar estruturas importantes existentes nos dados e, por outro lado, se o número é excessivo, ele pode vir a ter problemas de interpretação dos fatores. Existem, na literatura, vários critérios que auxiliam na determinação do número de fatores que, invariavelmente, quando empregados em um mesmo conjunto de dados, conduzem a resultados diferentes (HAIR, 1998).\nOs métodos de escolha, que se passou a descrever, têm caráter apenas indicativo, não existindo uma hierarquia entre eles.\n\nTécnica de Raiz Latente (Critério de Kaiser) : Esta técnica parte do princípio de que qualquer fator individual deve explicar a variância de pelo menos uma variável para que seja mantido para interpretação. Cada variável contribui com um valor 1 do autovalor total. Com efeito, apenas os fatores que têm raízes latentes ou autovalores maiores que 1 são considerados significantes e os demais fatores com autovalores menores do que 1 são considerados insignificantes e descartados (KAISER, 1958).\nCritério da % da Variância Explicada : O número é determinado de modo que o conjunto de fatores comuns explique uma porcentagem pré-definida da variabilidade global, por exemplo, desejamos explicar pelo menos 70% da variabilidade total dos dados.\nCritério Scree Test : È comum que a diferença de explicação entre os primeiros fatores de uma AF seja grande e que tenda a diminuir com o aumento no número de fatores. Por este critério, o número ótimo de fatores é obtido quando a variação da explicação entre fatores consecutivos passa a ser pequena. Com isso, de acordo com a Figura 5, pode-se verificar melhor o Critério Scree Test junto com o Critério da Raiz Latente ou de Kaiser.\nMétodos Inferenciais : Outros métodos foram desenvolvidos para os casos em que as variáveis originais seguem uma distribuição normal. Esses métodos consistem no desenvolvimento de testes estatísticos que se alicerçam na suposição de normalidade e, dessa forma, não são, em princípio, adequados à análise da maioria das escalas psicológicas. Apesar disso, esses métodos podem ser utilizados com um fim puramente indicativo, sendo que a significância obtida nessas situações não corresponde à realidade. Dentre esses testes destacamos o de Bartlett (Johnson e Wichern, 1992) que verifica a adequabilidade do modelo de AF estimado (pelo método da máxima verossimilhança) para representar a estrutura de dependência dos dados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#tamanho-de-amostra-para-aplicação-da-af",
    "href": "intro.html#tamanho-de-amostra-para-aplicação-da-af",
    "title": "1  Análise Fatorial - AF",
    "section": "1.11 Tamanho de Amostra para Aplicação da AF",
    "text": "1.11 Tamanho de Amostra para Aplicação da AF\nUma AF envolve a estimação de um grande número de parâmetros e, para que isso seja feito com um mínimo de qualidade, é necessário um tamanho amostral relativamente grande em comparação ao número de variáveis envolvidas. Há, na literatura estatística, uma série de sugestões para a escolha desse tamanho amostral.\nEm geral, essas opções baseiam-se na experiência pessoal dos diversos autores que, em alguns casos, sugerem um tamanho amostral da ordem de 20 vezes o número de variáveis envolvidas (ver Hair et al., 1995).\nReis (1997) e Hair et al. (1995) sugerem que o número de observações deva ser de no mínimo 5 vezes o número de variáveis, além disso, indicam que preferencialmente a análise seja feita com pelo menos 100 observações. Hair et al. (2005) enfatiza que ela não deve ser utilizada em amostras inferiores a 50 observações.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#métodos-de-estimação-dos-escores-fatoriais",
    "href": "intro.html#métodos-de-estimação-dos-escores-fatoriais",
    "title": "1  Análise Fatorial - AF",
    "section": "1.12 Métodos de Estimação dos Escores Fatoriais",
    "text": "1.12 Métodos de Estimação dos Escores Fatoriais\nQuando o objetivo final da análise de dados é a descrição e o entendimento da estrutura de correlação das variáveis, o que vimos sobre análise fatorial pode levar às respostas desejadas. Outras vezes, entretanto, os objetivos da pesquisa podem envolver análises posteriores aplicadas aos fatores identificados aos dados. È suposto que cada variável na amostra tenha um valor para cada um dos Fatores Comuns, que, como já foi dito, não são diretamente observáveis. Esses valores são os chamados Escores Fatoriais que posteriormente podem ser utilizados em outras análises conjuntamente. (BARROSO; ARTES, 2003).\nExistem na literatura vários métodos de estimação dos escores fatoriais para cada elemento amostral, tais como:\n\nMétodo dos Mínimos Quadrados Ponderados;\nMétodo de Bartlett;\nMétodo de Anderson Rubin;\nMétodo de Regressão.\n\nSendo o Método de Regressão usado para estimar os escores dos coeficientes dos fatores. Os escores gerados têm média 0 e variância igual ao quadrado da correlação múltipla entre os escores dos fatores estimados e os valores verdadeiros dos fatores. Os escores devem ser igualados com os fatores ortogonais.\nEm relação ao Método de Bartlett usado também para estimação dos escores dos coeficientes dos fatores. Os escores produzidos têm média de zero. A soma dos quadrados de um fator é feita sobre a extensão das variáveis minimizadas. Já o Método de Anderson Rubin é similar ao de Bartlett a diferença está em garantir a ortogonalidade dos fatores estimados. Os escores gerados têm uma média de 0, desvio padrão de 1,0 e são não correlatos.\n\n1.12.1 Padronização dos Escores Fatoriais\nAo realizar a análise sobre um conjunto de dados com variáveis com variâncias de magnitudes diferentes (provenientes de diversos setores), podemos estar introduzindo dificuldades na explicitação dessa dependência. Nos casos que existe uma grande diferença entre as variâncias das variáveis originais, sugere-se que a análise seja realizada sobre as variáveis padronizadas (Johnson, 1998).\nPara facilitar a comparabilidade dos índices de um município nos diversos grupos trasformou-se a base dos índices, ou seja, o escore fatorial foi padronizado para se obter valores positivos dos escores originais e permitir a hierarquização dos municípios, de tal forma que os valores do índice estimado estejam situados entre zero e um (Santana, 2007).\nA fórmula é a seguinte:\n\\[ Escore_{(Fatorial)_{(pad)}} =\\left(\\frac{F_{i}-F_{min}}{F_{max} - F_{mini}}\\right)   \\] Em que \\(F_{min}\\) e \\(F_{max}\\) são os valores de mínimo e máximo observado para os escores fatoriais associados aos municípios.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "intro.html#viabilidade-da-análise-de-fatores",
    "href": "intro.html#viabilidade-da-análise-de-fatores",
    "title": "1  Análise Fatorial - AF",
    "section": "1.13 Viabilidade da Análise de Fatores",
    "text": "1.13 Viabilidade da Análise de Fatores\nApresentam-se algumas medidas adicionais para aferir a viabilidade da aplicação de uma análise fatorial a um conjunto de dados. As principais medidas aplicadas foram: Matriz Anti-Imagem, Teste de Esfericidade de Bartlett, Teste de Kaiser-Meyer-Olkin (KMO) e Measure of Sampling Adequacy (MSA).\n\n1.13.1 Matriz Anti-imagem\nUma das premissas de uma análise fatorial é que exista uma estrutura de dependência clara entre as variáveis envolvidas. No modelo estudado, essa estrutura é expressa através da matriz de covariância ou de correlação. A existência de tal estrutura implica que uma variável pode, dentro de certos limites, ser prevista pelas demais. Para verificar esse fato, pode-se calcular os coeficientes de correlação parcial entre os pares de variáveis, eliminado o efeito das demais variáveis. Espera-se que os valores obtidos sejam baixos. A matriz anti-imagem é construída com esses coeficientes com sinais invertidos, muitas vezes, coloca-se na diagonal principal dessa matriz os indicadores MAS (Measure of Sampling Adequacy(MSA)) (BARROSO ; ARTES, 2003).\n\n\n1.13.2 Teste de Esfericidade de Bartlett\nEsse teste avalia a significância geral da matriz de correlação, ou seja, testa se todas as variáveis oriundas de diversos setores possuem uma possível relação em comum (Dillon; Goldstein, 1984; Reis, 2001).\nO teste de Bartlett testa as seguintes hipóteses nulas\n\\[\nH_{0}: R = I \\\\\nH_{1}: \\lambda_{1} = \\lambda_{2} = \\ldots = \\lambda_{p}\n\\]\nE a estatística do teste\n\\[\\chi^{2} = - [n-1-\\frac{1}{6}(2p+5)]\\sum_{i=1}^{p} ln|R|\\]\nEm que \\(|R|\\) é o determinante da matriz de correlação amostral, \\(\\lambda\\) é a variância explicada por cada fator, n é o número de observações e p é o número de variáveis.\nA estatística tem uma distribuição assintótica de \\(\\chi^{2}\\) com (0,5p(p-1)) graus de liberdade.\n\n\n1.13.3 Teste de Kaiser-Meyer-Olkin (KMO)\nEste teste é usado para compara as correlações parciais entre os pares de variáveis sem o efeito das demais, ou seja, testa se duas a duas variáveis possuem algum tipo de relação entre si (Dillon; Goldstein, 1984; Kaiser, 1970; Reis, 2001).\n\\[ KMO = \\left(\\frac{\\sum_{i}\\sum_{j}r_{ij}^{2}}{\\sum_{i}\\sum_{j}r_{ij}^{2}+\\sum_{i} \\sum_{j}a_{ij}^{2}}\\right) \\] Em que \\(r_{ij}\\) é o coeficiente de correlação observado entre as variáveis i e j e \\(a_{ij}\\) é o coeficiente de correlação parcial entre as mesmas variáveis que é, simultaneamente, uma estimativa das correlações entre os fatores, eliminado o efeito das demais variáveis. Os \\(a_{ij}\\) deverão assumir valores próximos de zero, uma vez que se pressupõe que os fatores são ortogonais entre si.\nConforme, (Kaiser; Rice, 1974), os valores do teste são classificados da seguinte forma:\nTabela 1. Classificação Geral do Teste de adequabilidade KMO segundo (Kaiser; Rice, 1974).\n\n\n\nKMO\nClassificação\n\n\n\n\n0.90 - 1.00\nExcelente\n\n\n0.80 – 0.90\nÓtimo\n\n\n0.70 – 0.80\nBom\n\n\n0.60 – 0.70\nRegular\n\n\n0.50 – 0.60\nRuim\n\n\n0.00 – 0.50\nInadequado\n\n\n\n\n\n1.13.4 Measure of Sampling Adequacy(MSA)\nEssa medida é bastante similar ao KMO. Novamente, deseja-se verificar a possibilidade de existir uma estrutura fatorial nos dados. Na verdade, a, MSA deve ser calculada separadamente para cada variável. O objetivo é verificar se uma dada variável pode ser explicada pelas demais (o que é esperado num modelo fatorial). Valores baixos de MSAi são indícios de que a respectiva variável pode ser retirada da análise sem maiores prejuízos (BARROSO; ARTES, 2003).\n\\[ MSA_{i} = \\left(\\frac{\\sum_{j=1}^{p}r_{ij}^{2}}{\\sum_{j=1}^{p}r_{ij}^{2} + \\sum_{j=1}^{p}a_{ij}^{2}} \\right)  \\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>**Análise Fatorial - AF**</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  }
]